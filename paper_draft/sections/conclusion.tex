\section{Conclusion}
\label{sec:conclusion}

We presented the first systematic empirical study of adversarial prompt propagation in multi-agent LLM collaborative systems, spanning 162 experimental conditions across three topologies, three complexity levels, and five injection types. Our central finding is that \textbf{injection technique, not network topology, is the dominant factor} governing propagation dynamics. Context poisoning via fabricated citations achieves 75\% marker persistence while role hijacking achieves less than 1\%, yet topology produces no significant differences ($p = 0.85$). Adversarial content undergoes significant semantic mutation as it traverses agent chains ($d = 0.83$), and all injection types cause measurable task degradation of 10--14\%.

These results carry concrete implications for the design of secure multi-agent systems. Defense efforts should prioritize content verification---particularly provenance tracking for factual claims and citations---over topology optimization. The most dangerous attacks are not the most aggressive (explicit instruction overrides) but the most plausible (fabricated evidence that blends with legitimate content). As multi-agent LLM systems scale to production deployments, the default model of inter-agent trust must be reconsidered: agents readily incorporate unverified claims from peers, enabling sophisticated contextual attacks.

Future work should extend this analysis to more capable models, longer agent chains, and topology-aware injection strategies. The marginal complexity--persistence correlation ($\rho = 0.14$) deserves investigation with larger sample sizes. Most critically, developing content-verification protocols that can detect semantically mutated adversarial content at agent communication boundaries remains an open and pressing challenge.
